
C:\spark-2.2.1-bin-hadoop2.7\spark-2.2.1-bin-hadoop2.7\bin>pyspark
Python 2.7.11 |Anaconda 2.5.0 (64-bit)| (default, Jan 29 2016, 14:26:21) [MSC v.1500 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
18/02/05 13:07:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/05 13:07:29 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.2.1
      /_/

Using Python version 2.7.11 (default, Jan 29 2016 14:26:21)
SparkSession available as 'spark'.

LOAD Text file
--------------------

>>> textFile=spark.read.text("C:\\spark-2.2.1-bin-hadoop2.7\\spark-2.2.1-bin-hadoop2.7\\README.md")

See the data loaded in the RDD
-------------------------------

>>> textFile.collect()
[Row(value=u'# Apache Spark'), Row(value=u''), Row(value=u'Spark is a fast and general cluster computing system for Big Data. It provides'), Row(value=u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'), Row(value=u'supports general computation graphs for data analysis. It also supports a'), Row(value=u'rich set of higher-level tools including Spark SQL for SQL and DataFrames,'), Row(value=u'MLlib for machine learning, GraphX for graph processing,'), Row(value=u'and Spark Streaming for stream processing.'), Row(value=u''), Row(value=u'<http://spark.apache.org/>'), Row(value=u''), Row(value=u''), Row(value=u'## Online Documentation'), Row(value=u''), Row(value=u'You can find the latest Spark documentation, including a programming'), Row(value=u'guide, on the [project web page](http://spark.apache.org/documentation.html).'), Row(value=u'This README file only contains basic setup instructions.'), Row(value=u''), Row(value=u'## Building Spark'), Row(value=u''), Row(value=u'Spark is built using [Apache Maven](http://maven.apache.org/).'), Row(value=u'To build Spark and its example programs, run:'), Row(value=u''), Row(value=u'    build/mvn -DskipTests clean package'), Row(value=u''), Row(value=u'(You do not need to do this if you downloaded a pre-built package.)'), Row(value=u''), Row(value=u'You can build Spark using more than one thread by using the -T option with Maven, see ["Parallel builds in Maven 3"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).'), Row(value=u'More detailed documentation is available from the project site, at'), Row(value=u'["Building Spark"](http://spark.apache.org/docs/latest/building-spark.html).'), Row(value=u''), Row(value=u'For general development tips, including info on developing Spark using an IDE, see ["Useful Developer Tools"](http://spark.apache.org/developer-tools.html).'), Row(value=u''), Row(value=u'## Interactive Scala Shell'), Row(value=u''), Row(value=u'The easiest way to start using Spark is through the Scala shell:'), Row(value=u''), Row(value=u'    ./bin/spark-shell'), Row(value=u''), Row(value=u'Try the following command, which should return 1000:'), Row(value=u''), Row(value=u'    scala> sc.parallelize(1 to 1000).count()'), Row(value=u''), Row(value=u'## Interactive Python Shell'), Row(value=u''), Row(value=u'Alternatively, if you prefer Python, you can use the Python shell:'), Row(value=u''), Row(value=u'    ./bin/pyspark'), Row(value=u''), Row(value=u'And run the following command, which should also return 1000:'), Row(value=u''), Row(value=u'    >>> sc.parallelize(range(1000)).count()'), Row(value=u''), Row(value=u'## Example Programs'), Row(value=u''), Row(value=u'Spark also comes with several sample programs in the `examples` directory.'), Row(value=u'To run one of them, use `./bin/run-example <class> [params]`. For example:'), Row(value=u''), Row(value=u'    ./bin/run-example SparkPi'), Row(value=u''), Row(value=u'will run the Pi example locally.'), Row(value=u''), Row(value=u'You can set the MASTER environment variable when running examples to submit'), Row(value=u'examples to a cluster. This can be a mesos:// or spark:// URL,'), Row(value=u'"yarn" to run on YARN, and "local" to run'), Row(value=u'locally with one thread, or "local[N]" to run locally with N threads. You'), Row(value=u'can also use an abbreviated class name if the class is in the `examples`'), Row(value=u'package. For instance:'), Row(value=u''), Row(value=u'    MASTER=spark://host:7077 ./bin/run-example SparkPi'), Row(value=u''), Row(value=u'Many of the example programs print usage help if no params are given.'), Row(value=u''), Row(value=u'## Running Tests'), Row(value=u''), Row(value=u'Testing first requires [building Spark](#building-spark). Once Spark is built, tests'), Row(value=u'can be run using:'), Row(value=u''), Row(value=u'    ./dev/run-tests'), Row(value=u''), Row(value=u'Please see the guidance on how to'), Row(value=u'[run tests for a module, or individual tests](http://spark.apache.org/developer-tools.html#individual-tests).'), Row(value=u''), Row(value=u'## A Note About Hadoop Versions'), Row(value=u''), Row(value=u'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported'), Row(value=u'storage systems. Because the protocols have changed in different versions of'), Row(value=u'Hadoop, you must build Spark against the same version that your cluster runs.'), Row(value=u''), Row(value=u'Please refer to the build documentation at'), Row(value=u'["Specifying the Hadoop Version"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)'), Row(value=u'for detailed guidance on building for a particular distribution of Hadoop, including'), Row(value=u'building for particular Hive and Hive Thriftserver distributions.'), Row(value=u''), Row(value=u'## Configuration'), Row(value=u''), Row(value=u'Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)'), Row(value=u'in the online documentation for an overview on how to configure Spark.'), Row(value=u''), Row(value=u'## Contributing'), Row(value=u''), Row(value=u'Please review the [Contribution to Spark guide](http://spark.apache.org/contributing.html)'), Row(value=u'for information on how to get started contributing to the project.')]

Read the 1st dataframe
-----------------------
>>> textFile.first()
Row(value=u'# Apache Spark')

Count the no of values in the RDD
-----------------------------------
>>> textFile.count()
103

Filter lines in RDD containing work 'Spark'
-----------------------------------------------
>>> linesWithSpark=textFile.filter(textFile.value.contains("Spark"))
>>> linesWithSpark.collect()
[Row(value=u'# Apache Spark'), Row(value=u'Spark is a fast and general cluster computing system for Big Data. It provides'), Row(value=u'rich set of higher-level tools including Spark SQL for SQL and DataFrames,'), Row(value=u'and Spark Streaming for stream processing.'), Row(value=u'You can find the latest Spark documentation, including a programming'), Row(value=u'## Building Spark'), Row(value=u'Spark is built using [Apache Maven](http://maven.apache.org/).'), Row(value=u'To build Spark and its example programs, run:'), Row(value=u'You can build Spark using more than one thread by using the -T option with Maven, see ["Parallel builds in Maven 3"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).'), Row(value=u'["Building Spark"](http://spark.apache.org/docs/latest/building-spark.html).'), Row(value=u'For general development tips, including info on developing Spark using an IDE, see ["Useful Developer Tools"](http://spark.apache.org/developer-tools.html).'), Row(value=u'The easiest way to start using Spark is through the Scala shell:'), Row(value=u'Spark also comes with several sample programs in the `examples` directory.'), Row(value=u'    ./bin/run-example SparkPi'), Row(value=u'    MASTER=spark://host:7077 ./bin/run-example SparkPi'), Row(value=u'Testing first requires [building Spark](#building-spark). Once Spark is built, tests'), Row(value=u'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported'), Row(value=u'Hadoop, you must build Spark against the same version that your cluster runs.'), Row(value=u'in the online documentation for an overview on how to configure Spark.'), Row(value=u'Please review the [Contribution to Spark guide](http://spark.apache.org/contributing.html)')]
>>> linesWithSpark.count()
20

Find line with the most words
-----------------------------

>>> from pyspark.sql.functions import *
>>> textFile.select(size(split(textFile.value, "\s+")).name("numWords")).agg(max(col("numWords"))).collect()
[Row(max(numWords)=22)]

Word Frequency
---------------

>>> wordCounts=textFile.select(explode(split(textFile.value, "\s+")).alias("word")).groupBy("word").count()
>>> wordCounts.collect()
[Row(word=u'online', count=1), Row(word=u'graphs', count=1), Row(word=u'["Parallel', count=1), Row(word=u'["Building', count=1), Row(word=u'thread', count=1), Row(word=u'documentation', count=3), Row(word=u'command,', count=2), Row(word=u'abbreviated', count=1), Row(word=u'overview', count=1), Row(word=u'rich', count=1), Row(word=u'set', count=2), Row(word=u'-DskipTests', count=1), Row(word=u'name', count=1), Row(word=u'page](http://spark.apache.org/documentation.html).', count=1), Row(word=u'["Specifying', count=1), Row(word=u'stream', count=1), Row(word=u'run:', count=1), Row(word=u'not', count=1), Row(word=u'programs', count=2), Row(word=u'tests', count=2), Row(word=u'./dev/run-tests', count=1), Row(word=u'will', count=1), Row(word=u'[run', count=1), Row(word=u'particular', count=2), Row(word=u'option', count=1), Row(word=u'Alternatively,', count=1), Row(word=u'by', count=1), Row(word=u'must', count=1), Row(word=u'using', count=5), Row(word=u'you', count=4), Row(word=u'MLlib', count=1), Row(word=u'DataFrames,', count=1), Row(word=u'variable', count=1), Row(word=u'Note', count=1), Row(word=u'core', count=1), Row(word=u'more', count=1), Row(word=u'protocols', count=1), Row(word=u'guidance', count=2), Row(word=u'shell:', count=2), Row(word=u'can', count=7), Row(word=u'site,', count=1), Row(word=u'systems.', count=1), Row(word=u'Maven', count=1), Row(word=u'[building', count=1), Row(word=u'configure', count=1), Row(word=u'for', count=12), Row(word=u'README', count=1), Row(word=u'Interactive', count=2), Row(word=u'how', count=3), Row(word=u'[Configuration', count=1), Row(word=u'Hive', count=2), Row(word=u'system', count=1), Row(word=u'provides', count=1), Row(word=u'Hadoop-supported', count=1), Row(word=u'pre-built', count=1), Row(word=u'["Useful', count=1), Row(word=u'directory.', count=1), Row(word=u'Example', count=1), Row(word=u'example', count=3), Row(word=u'one', count=3), Row(word=u'MASTER', count=1), Row(word=u'in', count=6), Row(word=u'library', count=1), Row(word=u'Spark.', count=1), Row(word=u'contains', count=1), Row(word=u'Configuration', count=1), Row(word=u'programming', count=1), Row(word=u'with', count=4), Row(word=u'contributing', count=1), Row(word=u'downloaded', count=1), Row(word=u'1000).count()', count=1), Row(word=u'comes', count=1), Row(word=u'machine', count=1), Row(word=u'Tools"](http://spark.apache.org/developer-tools.html).', count=1), Row(word=u'Version"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', count=1), Row(word=u'building', count=2), Row(word=u'params', count=1), Row(word=u'Guide](http://spark.apache.org/docs/latest/configuration.html)', count=1), Row(word=u'given.', count=1), Row(word=u'be', count=2), Row(word=u'same', count=1), Row(word=u'than', count=1), Row(word=u'Programs', count=1), Row(word=u'locally', count=2), Row(word=u'using:', count=1), Row(word=u'fast', count=1), Row(word=u'[Apache', count=1), Row(word=u'your', count=1), Row(word=u'optimized', count=1), Row(word=u'Developer', count=1), Row(word=u'R,', count=1), Row(word=u'should', count=2), Row(word=u'graph', count=1), Row(word=u'package', count=1), Row(word=u'-T', count=1), Row(word=u'[project', count=1), Row(word=u'project', count=1), Row(word=u'`examples`', count=2), Row(word=u'versions', count=1), Row(word=u'Spark](#building-spark).', count=1), Row(word=u'general', count=3), Row(word=u'other', count=1), Row(word=u'learning,', count=1), Row(word=u'when', count=1), Row(word=u'submit', count=1), Row(word=u'Apache', count=1), Row(word=u'1000:', count=2), Row(word=u'detailed', count=2), Row(word=u'About', count=1), Row(word=u'is', count=6), Row(word=u'on', count=7), Row(word=u'scala>', count=1), Row(word=u'print', count=1), Row(word=u'use', count=3), Row(word=u'different', count=1), Row(word=u'following', count=2), Row(word=u'SparkPi', count=2), Row(word=u'refer', count=2), Row(word=u'./bin/run-example', count=2), Row(word=u'data', count=1), Row(word=u'Tests', count=1), Row(word=u'Versions', count=1), Row(word=u'Data.', count=1), Row(word=u'processing.', count=1), Row(word=u'its', count=1), Row(word=u'basic', count=1), Row(word=u'latest', count=1), Row(word=u'only', count=1), Row(word=u'<class>', count=1), Row(word=u'have', count=1), Row(word=u'runs.', count=1), Row(word=u'You', count=4), Row(word=u'tips,', count=1), Row(word=u'project.', count=1), Row(word=u'developing', count=1), Row(word=u'YARN,', count=1), Row(word=u'It', count=2), Row(word=u'"local"', count=1), Row(word=u'processing,', count=1), Row(word=u'built', count=1), Row(word=u'Pi', count=1), Row(word=u'thread,', count=1), Row(word=u'A', count=1), Row(word=u'APIs', count=1), Row(word=u'Scala,', count=1), Row(word=u'file', count=1), Row(word=u'computation', count=1), Row(word=u'Once', count=1), Row(word=u'find', count=1), Row(word=u'the', count=24), Row(word=u'To', count=2), Row(word=u'sc.parallelize(1', count=1), Row(word=u'uses', count=1), Row(word=u'N', count=1), Row(word=u'programs,', count=1), Row(word=u'"yarn"', count=1), Row(word=u'see', count=3), Row(word=u'./bin/pyspark', count=1), Row(word=u'return', count=2), Row(word=u'computing', count=1), Row(word=u'Java,', count=1), Row(word=u'from', count=1), Row(word=u'Because', count=1), Row(word=u'cluster', count=2), Row(word=u'Streaming', count=1), Row(word=u'More', count=1), Row(word=u'analysis.', count=1), Row(word=u'Maven](http://maven.apache.org/).', count=1), Row(word=u'cluster.', count=1), Row(word=u'Running', count=1), Row(word=u'Please', count=4), Row(word=u'talk', count=1), Row(word=u'distributions.', count=1), Row(word=u'guide,', count=1), Row(word=u'tests](http://spark.apache.org/developer-tools.html#individual-tests).', count=1), Row(word=u'"local[N]"', count=1), Row(word=u'Try', count=1), Row(word=u'and', count=9), Row(word=u'do', count=2), Row(word=u'Scala', count=2), Row(word=u'class', count=2), Row(word=u'build', count=4), Row(word=u'3"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).', count=1), Row(word=u'setup', count=1), Row(word=u'need', count=1), Row(word=u'spark://', count=1), Row(word=u'Hadoop,', count=2), Row(word=u'Thriftserver', count=1), Row(word=u'are', count=1), Row(word=u'requires', count=1), Row(word=u'package.', count=1), Row(word=u'clean', count=1), Row(word=u'sc.parallelize(range(1000)).count()', count=1), Row(word=u'high-level', count=1), Row(word=u'SQL', count=2), Row(word=u'against', count=1), Row(word=u'of', count=5), Row(word=u'through', count=1), Row(word=u'review', count=1), Row(word=u'package.)', count=1), Row(word=u'Python,', count=2), Row(word=u'easiest', count=1), Row(word=u'no', count=1), Row(word=u'Testing', count=1), Row(word=u'several', count=1), Row(word=u'help', count=1), Row(word=u'The', count=1), Row(word=u'sample', count=1), Row(word=u'MASTER=spark://host:7077', count=1), Row(word=u'Big', count=1), Row(word=u'examples', count=2), Row(word=u'an', count=4), Row(word=u'#', count=1), Row(word=u'Online', count=1), Row(word=u'including', count=4), Row(word=u'usage', count=1), Row(word=u'Python', count=2), Row(word=u'at', count=2), Row(word=u'development', count=1), Row(word=u'Spark"](http://spark.apache.org/docs/latest/building-spark.html).', count=1), Row(word=u'IDE,', count=1), Row(word=u'way', count=1), Row(word=u'Contributing', count=1), Row(word=u'get', count=1), Row(word=u'that', count=2), Row(word=u'##', count=9), Row(word=u'For', count=3), Row(word=u'prefer', count=1), Row(word=u'This', count=2), Row(word=u'build/mvn', count=1), Row(word=u'builds', count=1), Row(word=u'running', count=1), Row(word=u'web', count=1), Row(word=u'run', count=7), Row(word=u'locally.', count=1), Row(word=u'Spark', count=16), Row(word=u'URL,', count=1), Row(word=u'a', count=8), Row(word=u'higher-level', count=1), Row(word=u'tools', count=1), Row(word=u'if', count=4), Row(word=u'available', count=1), Row(word=u'', count=47), Row(word=u'Documentation', count=1), Row(word=u'this', count=1), Row(word=u'(You', count=1), Row(word=u'>>>', count=1), Row(word=u'information', count=1), Row(word=u'info', count=1), Row(word=u'<http://spark.apache.org/>', count=1), Row(word=u'Shell', count=2), Row(word=u'environment', count=1), Row(word=u'built,', count=1), Row(word=u'module,', count=1), Row(word=u'them,', count=1), Row(word=u'`./bin/run-example', count=1), Row(word=u'instance:', count=1), Row(word=u'first', count=1), Row(word=u'[Contribution', count=1), Row(word=u'guide](http://spark.apache.org/contributing.html)', count=1), Row(word=u'documentation,', count=1), Row(word=u'[params]`.', count=1), Row(word=u'mesos://', count=1), Row(word=u'engine', count=1), Row(word=u'GraphX', count=1), Row(word=u'Maven,', count=1), Row(word=u'example:', count=1), Row(word=u'HDFS', count=1), Row(word=u'or', count=3), Row(word=u'to', count=17), Row(word=u'Hadoop', count=3), Row(word=u'individual', count=1), Row(word=u'also', count=4), Row(word=u'changed', count=1), Row(word=u'started', count=1), Row(word=u'./bin/spark-shell', count=1), Row(word=u'threads.', count=1), Row(word=u'supports', count=2), Row(word=u'storage', count=1), Row(word=u'version', count=1), Row(word=u'instructions.', count=1), Row(word=u'Building', count=1), Row(word=u'start', count=1), Row(word=u'Many', count=1), Row(word=u'which', count=2), Row(word=u'And', count=1), Row(word=u'distribution', count=1)]

Caching 
-------

>>> linesWithSpark.cache()
DataFrame[value: string]
>>> linesWithSpark.count()
20

